<html>
<head>
    <title>Goodoids Backyard</title>
</head>
<body>
    <h1>
        My Way to AI
    </h1>
Technology startups today work very well for making a super-efficient piston engine, but they are unlikely to fund the kind of open-ended R+D required to develop a jet engine.
    <p>
        <a href="http://neuralnetworksanddeeplearning.com/chap3.html">This's why I give it up</a>
        "Well, that's easy to fix," you might say, "just decrease the learning rate and regularization hyper-parameters". Unfortunately, you don't a priori know those are the hyper-parameters you need to adjust. Maybe the real problem is that our 30 hidden neuron network will never work well, no matter how the other hyper-parameters are chosen? Maybe we really need at least 100 hidden neurons? Or 300 hidden neurons? Or multiple hidden layers? Or a different approach to encoding the output? Maybe our network is learning, but we need to train for more epochs? Maybe the mini-batches are too small? Maybe we'd do better switching back to the quadratic cost function? Maybe we need to try a different approach to weight initialization? And so on, on and on and on. It's easy to feel lost in hyper-parameter space. This can be particularly frustrating if your network is very large, or uses a lot of training data, since you may train for hours or days or weeks, only to get no result. If the situation persists, it damages your confidence. Maybe neural networks are the wrong approach to your problem? Maybe you should quit your job and take up beekeeping?
</body>
</html>
